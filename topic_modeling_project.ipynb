{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic modeling using neural attention for aspect extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plan for the project:\n",
    "1. Text exploration.\n",
    "2. Text preprocessing using different tools, including byte pair encoding (BPE).\n",
    "3. Training model.\n",
    "4. Evaluation of results.    \n",
    "    4.1. Evaluation by coherense score.    \n",
    "    4.2. Evaluation by text classifacation.\n",
    "5. Discription of future steps for the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ivan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\ivan\\\\YandexDisk\\\\DS\\\\NLP course Huawei\\\\Project NLP\\\\data'"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from importlib import reload \n",
    "import text_preprocess\n",
    "from text_preprocess import  preprocess , remove_rare_words, process_words_for_LDA, text_to_id\n",
    "\n",
    "from spacy.lemmatizer import Lemmatizer, Lookups\n",
    "from spacy.lang.ru import RussianLemmatizer\n",
    "from pprint import pprint\n",
    "# Gensim\n",
    "import gensim, logging, warnings\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import lemmatize, simple_preprocess\n",
    "from gensim.models import CoherenceModel, LdaModel, LdaMulticore\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import pymorphy2\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt    \n",
    "    \n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('russian')\n",
    "stop.remove('не')\n",
    "\n",
    "import model\n",
    "from model import get_aspect_matrix\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Graphs\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from model import Net, MaxMarginLoss\n",
    "\n",
    "\n",
    "# папка текущего файла\n",
    "\n",
    "PATH = os.path.abspath(os.getcwd())\n",
    "\n",
    "# Папка исходных файлов\n",
    "DATA_PATH = os.path.join(PATH,'data')\n",
    "\n",
    "DATA_PATH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ivan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Extending stop words: 458it [00:00, 458142.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stop words:  609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'model' from 'C:\\\\Users\\\\ivan\\\\YandexDisk\\\\DS\\\\NLP course Huawei\\\\Project NLP\\\\model.py'>"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(text_preprocess)\n",
    "reload(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset consists of 2 separate dataframes with positive and negative twits. Each dataframe consist more then 100000 rows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading **positive** twits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df = pd.read_csv(os.path.join(DATA_PATH,'positive.csv'), header = None, sep=';')\n",
    "\n",
    "# column titles\n",
    "cols = ['id',\n",
    "       'tdate',\n",
    "       'tmane',\n",
    "       'ttext',\n",
    "       'ttype',\n",
    "       'trep',\n",
    "       'trtw',\n",
    "       'tfav',\n",
    "       'tstcount',\n",
    "       'tfol',\n",
    "       'tfrien',\n",
    "       'listcount'\n",
    "       ]\n",
    "\n",
    "pos_df.columns = cols\n",
    "pos_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading **negative** twits:    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_df = pd.read_csv(os.path.join(DATA_PATH,'negative.csv'), header = None, sep=';')\n",
    "\n",
    "neg_df.columns = cols\n",
    "neg_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For further work we will concatenate dataframes with only text and its sentiment, and mix rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df['sentiment'] = 1\n",
    "neg_df['sentiment'] = 0\n",
    "train_df = pd.concat([pos_df, neg_df]).sample(frac=1, random_state=12).reset_index(drop=True)\n",
    "train_df = train_df[['ttext', 'sentiment']]\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see corpus is very raw and need to be preprocessed. Steps for preprocessing are described below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data exploration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have look at length of the twits and amount of words in twit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['twit_len'] = train_df['ttext'].map(len)\n",
    "\n",
    "# filter text list if there are no spaces\n",
    "train_df['words_num'] = [len(list(filter(lambda x: x!='' , text.split(' ')))) for text in train_df['ttext']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['twit_len'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['twit_len'].plot(kind='hist', bins=50, figsize=(10,5));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['words_num'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['words_num'].plot(kind='hist', bins=50, figsize=(10,5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step the prepering processes are:\n",
    "1. Convert characters to lower case.\n",
    "2. Remove words with characters length less than 3.\n",
    "3. Remove non-alphabetic symbols.\n",
    "4. Byte pair encoding of unknown words.\n",
    "5. Convert different emoticons to several types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#reload(text_preprocess)\n",
    "\n",
    "corpus = train_df['ttext']\n",
    "\n",
    "train_df['clean_text'] = preprocess(corpus)\n",
    "\n",
    "train_df['clean_twit_len'] = train_df['clean_text'].map(len)\n",
    "\n",
    "# filter text list if there are no spaces\n",
    "train_df['clean_words_num'] = [len(list(filter(lambda x: x!='' , text.split(' ')))) for text in train_df['clean_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['clean_twit_len'].plot(kind='hist', bins=50, figsize=(10,5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see distribution became more normal than it was before preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['clean_words_num'].plot(kind='hist', bins=50, figsize=(10,5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing rows with number of words more than 20:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df= train_df[train_df['clean_words_num']<20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['clean_words_num'].plot(kind='hist', bins=50, figsize=(10,5));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will remove words that appear very rare in the corpus since they are unique and can not give us enough information about topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = train_df['clean_text'].values\n",
    "\n",
    "# number is min count values to cut words\n",
    "corpus_clean = remove_rare_words(corpus, 25, 30)# num of appearence of rare words and n most frequent tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating dataframe from dictionary to check result how much words in dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = corpora.Dictionary(corpus_clean)\n",
    "\n",
    "#dictionary word:word id\n",
    "dict_words = dic.token2id\n",
    "\n",
    "#dictionary word id: number of word\n",
    "dic_id_nums = dic.cfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_df = pd.DataFrame.from_dict({'token':list(dict_words.keys()), 'count_word': list(dic_id_nums.values())})\n",
    "len(dict_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dict_df.sort_values(by='count_word', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_df[dict_df['token']=='михаэль']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Text preprocess for LDA model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating bigrams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_preprocessed = process_words_for_LDA(corpus_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(corpus_preprocessed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean dataset if there are empty docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_preprocessed = [doc for doc in corpus_preprocessed if len(doc)!=0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check length of corpus after cleaning:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(corpus_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = corpora.Dictionary(corpus_preprocessed)\n",
    "\n",
    "#dictionary word:word id\n",
    "dict_words = dic.token2id\n",
    "\n",
    "#dictionary word id: number of word\n",
    "dic_id_nums = dic.cfs\n",
    "\n",
    "dict_df = pd.DataFrame.from_dict({'token':list(dict_words.keys()), 'count_word': list(dic_id_nums.values())})\n",
    "len(dict_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_df.sort_values(by='count_word', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text with indexes of words in vocablary instead of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed_text = text_to_id(corpus_preprocessed, word2id)\n",
    "indexed_text[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.Model implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(corpus_preprocessed)\n",
    "\n",
    "# Create Corpus: Term Document Frequency\n",
    "corpus_freq = [id2word.doc2bow(text) for text in corpus_preprocessed]\n",
    "\n",
    "# Create word-id dictionary\n",
    "word2id = id2word.token2id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find optimum number of topics by calculating coherense coefficient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in tqdm(range(start, limit, step), desc = 'Model working'):\n",
    "        model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=num_topics, \n",
    "                                           random_state=12,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=10,\n",
    "                                           passes=3,\n",
    "                                           alpha='symmetric',\n",
    "                                           iterations=50,\n",
    "                                           per_word_topics=True)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Num of topics to choose\n",
    "start=2\n",
    "limit=20\n",
    "step=2\n",
    "\n",
    "\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=id2word, \n",
    "                                                        corpus=corpus_freq, \n",
    "                                                        texts=corpus_for_LDA, \n",
    "                                                        start=start, \n",
    "                                                        limit=limit, \n",
    "                                                        step=step)\n",
    "# Show graph\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the coherence scores\n",
    "for m, cv in zip(x, coherence_values):\n",
    "    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_cv = max(coherence_values)\n",
    "best_lda_model = model_list[coherence_values.index(max_cv)]\n",
    "best_lda_model.save('best_lda_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "# Build LDA model\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus_freq,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=12, \n",
    "                                           random_state=14,\n",
    "                                           #update_every=1,\n",
    "                                           chunksize=10,\n",
    "                                           passes=3,\n",
    "                                           alpha='symmetric',\n",
    "                                           iterations=100,\n",
    "                                           per_word_topics=True,\n",
    "                                           dtype=np.float64,\n",
    "                                           workers = 3)\n",
    "\n",
    "pprint(lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus_freq, dictionary=id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result of topic modeling with LDA:\n",
    "1. Optimizing number of topics by higher coherence score was reached optimum number 14. Maximum score is 0.44.\n",
    "2. For some group of words we can form topic, but for most of them it is not vivid.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Attention based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(id2word)\n",
    "EMB_SIZE = 200\n",
    "N_ASPECTS = 14\n",
    "MAX_LEN = 20\n",
    "NEG_SAMPLES = 20\n",
    "BATCH_SIZE = 50\n",
    "seed = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5822"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2v_model = Word2Vec(sentences=[one_txt], size=200, window=10, min_count=10, workers=3, negative=5)\n",
    "w2v_model = Word2Vec(sentences=corpus_preprocessed, size=200, window=5, min_count=1, workers=3, batch_words=10)\n",
    "\n",
    "if not os.path.exists(PATH + '/pre_trained_model/'):\n",
    "    os.makedirs(PATH + '/pre_trained_model/')\n",
    "\n",
    "w2v_model.save(PATH + '/pre_trained_model/model_param_my')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how good enough word2vec model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-275-d8dbf98a5e9e>:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  w2v_model.most_similar('кушать', topn = 20)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('поесть', 0.9762868285179138),\n",
       " ('жрать', 0.9742609262466431),\n",
       " ('выпить', 0.9601010084152222),\n",
       " ('кроватка', 0.946843147277832),\n",
       " ('тренировка', 0.9462007284164429),\n",
       " ('насморк', 0.942147970199585),\n",
       " ('постель', 0.9413187503814697),\n",
       " ('вечером', 0.9405970573425293),\n",
       " ('завтрак', 0.9370702505111694),\n",
       " ('устать', 0.933862566947937),\n",
       " ('вода', 0.9317432045936584),\n",
       " ('пошлый', 0.9311273097991943),\n",
       " ('пипец', 0.9307984113693237),\n",
       " ('сесть', 0.9287680983543396),\n",
       " ('ранний', 0.928617000579834),\n",
       " ('живот', 0.9283583164215088),\n",
       " ('целый', 0.9282732009887695),\n",
       " ('посидеть', 0.9269917011260986),\n",
       " ('одеяло', 0.9269794225692749),\n",
       " ('доехать', 0.9268539547920227)]"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.most_similar('кушать', topn = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_embeddings = np.zeros(shape=[VOCAB_SIZE, EMB_SIZE], dtype=np.float64)\n",
    "\n",
    "for word, vec in w2v_model.wv.vocab.items():\n",
    "    vector = w2v_model.wv.get_vector(word)\n",
    "    idx = word2id.get(word)\n",
    "    np_embeddings[idx] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5822"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(w2v_model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_embeddings_norm = np_embeddings / np.linalg.norm(np_embeddings, axis=-1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5822, 200)"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_embeddings_norm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot word embeddings projected on 3D space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_plot(model):\n",
    "    \"Creates and TSNE model and plots it\"\n",
    "    labels = []\n",
    "    tokens = []\n",
    "\n",
    "    for word in model.wv.vocab:\n",
    "        tokens.append(model.wv[word])\n",
    "        labels.append(word)\n",
    "    \n",
    "    tokens_norm = tokens/ np.linalg.norm(tokens, axis=-1, keepdims=True)\n",
    "    \n",
    "    tsne_model = TSNE(perplexity=40, n_components=3, init='pca', n_iter=2500, random_state=23)\n",
    "    new_values = tsne_model.fit_transform(tokens_norm)\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    z = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        z.append(value[2])\n",
    "    plt.figure(figsize=(16, 16)).gca(projection='3d')\n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i], z[i],cmap='tab10')\n",
    "       \n",
    "        #ax.set_xlabel('pca-one')\n",
    "        #ax.set_ylabel('pca-two')\n",
    "        #ax.set_zlabel('pca-three')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_plot(w2v_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Aspect matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspect_matrix = get_aspect_matrix(np_embeddings, N_ASPECTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Pretrained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x22fd4d2c430>"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "#torch.cuda.manual_seed(seed)\n",
    "#torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "TORCH_DEVICE ='cpu'# 'cuda' # 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device(TORCH_DEVICE)\n",
    "#torch.set_default_tensor_type(torch.cuda.FloatTensor) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5822, 200])"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_embeddings = torch.FloatTensor(np_embeddings_norm)\n",
    "pretrained_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Text to torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "def texts_to_seq(texts_to_ids, maxlen=MAX_LEN):\n",
    "    pad_token = 0\n",
    "    sequences = [\n",
    "        [pad_token]*(maxlen - len(txt)) + txt[-maxlen:]\n",
    "        for txt in texts_to_ids\n",
    "        if len(txt) != 0\n",
    "    ]\n",
    "    return torch.tensor(sequences, dtype=torch.long, device=TORCH_DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([216133, 20])"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_texts = texts_to_seq(indexed_text, maxlen=MAX_LEN)\n",
    "padded_texts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_neg_loader = DataLoader(\n",
    "    dataset=PosNegDataset(padded_texts, neg_size=NEG_SAMPLES),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "#     num_workers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x22f98f22910>"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_neg_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "\nThe NVIDIA driver on your system is too old (found version 8000).\nPlease update your GPU driver by downloading and installing a new\nversion from the URL: http://www.nvidia.com/Download/index.aspx\nAlternatively, go to: https://pytorch.org to install\na PyTorch version that has been compiled with your version\nof the CUDA driver.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-319-6bee2190cd9f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m model = Net(vocab_size=VOCAB_SIZE, emb_dim=EMB_SIZE, maxlen=MAX_LEN, n_aspects=N_ASPECTS, \n\u001b[0m\u001b[0;32m      2\u001b[0m             pretrained_embeddings=pretrained_embeddings, aspect_matrix=aspect_matrix)\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Parameters:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnelement\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\YandexDisk\\DS\\NLP course Huawei\\Project NLP\\model.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, vocab_size, emb_dim, maxlen, n_aspects, pretrained_embeddings, aspect_matrix)\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaxlen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_aspects\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn_aspects\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maspect_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maspect_matrix\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTORCH_DEVICE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m         self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=True, \n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    184\u001b[0m             raise RuntimeError(\n\u001b[0;32m    185\u001b[0m                 \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\n\u001b[1;32m--> 186\u001b[1;33m         \u001b[0m_check_driver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    187\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_cudart\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m             raise AssertionError(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36m_check_driver\u001b[1;34m()\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[1;31m# TODO: directly link to the alternative bin that needs install\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m             raise AssertionError(\"\"\"\n\u001b[0m\u001b[0;32m     72\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mNVIDIA\u001b[0m \u001b[0mdriver\u001b[0m \u001b[0mon\u001b[0m \u001b[0myour\u001b[0m \u001b[0msystem\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mtoo\u001b[0m \u001b[0mold\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfound\u001b[0m \u001b[0mversion\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[0mPlease\u001b[0m \u001b[0mupdate\u001b[0m \u001b[0myour\u001b[0m \u001b[0mGPU\u001b[0m \u001b[0mdriver\u001b[0m \u001b[0mby\u001b[0m \u001b[0mdownloading\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0minstalling\u001b[0m \u001b[0ma\u001b[0m \u001b[0mnew\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: \nThe NVIDIA driver on your system is too old (found version 8000).\nPlease update your GPU driver by downloading and installing a new\nversion from the URL: http://www.nvidia.com/Download/index.aspx\nAlternatively, go to: https://pytorch.org to install\na PyTorch version that has been compiled with your version\nof the CUDA driver."
     ]
    }
   ],
   "source": [
    "model = Net(vocab_size=VOCAB_SIZE, emb_dim=EMB_SIZE, maxlen=MAX_LEN, n_aspects=N_ASPECTS, \n",
    "            pretrained_embeddings=pretrained_embeddings, aspect_matrix=aspect_matrix)\n",
    "\n",
    "print(model)\n",
    "print(\"Parameters:\", sum([param.nelement() for param in model.parameters()]))\n",
    "\n",
    "model.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Testing model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from model import PosNegDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Results evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
